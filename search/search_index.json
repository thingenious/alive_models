{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Alive Models Server","text":"<p>A PyTriton server of multiple models for different tasks like Automatic Speech Recognition, Speech Emotion Recognition, Facial Emotion Recognition, and Natural Language Processing (Sentiment Analysis) Small chunks of audio and text, as well as single images, can be sent to the server to get the results of the models. The server can be deployed using Podman/Docker compose or Kubernetes and exposed to the internet using a reverse proxy like Nginx. The exposed REST and gRPC endpoints follow the Predict Inference Protocol, version 2.</p>"},{"location":"#demo","title":"Demo","text":"<p>One or more of the models can be used to analyze audio, text, and images. A short capture of an example that uses all the models can be seen below:</p>"},{"location":"#models","title":"Models","text":"<p>The models that are currently being served are:</p> <ul> <li>ASR (Automatic Speech Recognition): distil-large-v3 using faster-whisper</li> <li>FER (Facial Emotion Recognition): deepface with yolov8 for face detection.</li> <li>NLP (Natural Language Processing): SamLowe/roberta-base-go_emotions-onnx</li> <li>SER (Speech Emotion Recognition): hughlan1214/Speech_Emotion_Recognition_wav2vec2-large-xlsr-53_240304_SER_fine-tuned2.0</li> </ul>"},{"location":"api/","title":"API","text":"<p>For details on all the available endpoints, you can refer to the standard inference protocols and the PyTriton docs.</p>"},{"location":"api/#rest-examples","title":"REST Examples","text":"<p>Some examples of how to use the REST API are shown below.</p>"},{"location":"api/#asr-automatic-speech-recognition","title":"ASR (Automatic Speech Recognition)","text":"<p>URL: <code>/v2/models/asr/versions/1/infer</code> Input Names:</p> <ul> <li><code>data</code>: Base64 encoded audio data (of wav in 16-bit PCM format)</li> </ul> <p>Output Names:</p> <ul> <li><code>results</code>: JSON dumped segments of the audio with their text, the timestamps and words</li> </ul> <pre><code>import base64\nimport json\nimport httpx\n\nheaders = {\"Content-Type\": \"application/json\"}\naudio_data = open(\"path/to/chunk.wav\", \"rb\").read()\nb64_audio_data = base64.b64encode(audio_data).decode(\"utf-8\")\nrequest_data = json.dumps(\n    {\n        \"id\": \"1\",\n        \"inputs\": [\n            {\"name\": \"data\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [b64_audio_data]},\n        ],\n    }\n)\nurl = \"http://localhost:8000/v2/models/asr/versions/1/infer\"\nresponse = httpx.post(\n    url,\n    headers=headers,\n    content=request_data,\n    timeout=60,\n)\n\nprint(response.json())\n\n# Example output:\n# {\n#     \"id\": \"1\",\n#     \"model_name\": \"asr\",\n#     \"model_version\": \"1\",\n#     \"outputs\": [\n#         {\n#             \"name\": \"results\",\n#             \"datatype\": \"BYTES\",\n#             \"shape\": [],\n#             \"data\": [\"[{\\\"id\\\": 1, \\\"seek\\\": 10, \\\"start\\\": 0.0, \\\"end\\\": 1.0, \\\"text\\\": \\\"hello\\\", \\\"tokens\\\": [1, 2, 3, 4, 5], \\\"temperature\\\": 0.0, \\\"avg_logprob\\\": -0.1140624976158142, \\\"compression_ratio\\\": 1.0, \\\"no_speech_prob\\\": 0.0011053085327148438, \\\"words\\\": [{\\\"start\\\": 0.0, \\\"end\\\": 0.6, \\\"word\\\": \\\"Hello\\\", \\\"probability\\\": 0.96728515625}, {\\\"start\\\": 0.7, \\\"end\\\": 1.2, \\\"word\\\": \\\"World\\\", \\\"probability\\\": 0.99755859375]\"]\n#         }\n#     ]\n# }\n</code></pre>"},{"location":"api/#ser-speech-emotion-recognition","title":"SER (Speech Emotion Recognition)","text":"<p>URL: <code>/v2/models/ser/versions/1/infer</code></p> <p>Input Names:</p> <ul> <li><code>data</code>: Base64 encoded audio data (of wav in 16-bit PCM format)</li> </ul> <p>Output Names:</p> <ul> <li><code>results</code>: JSON dumped predictions with the <code>label</code> and <code>score</code> for each emotion</li> </ul> <pre><code># ...\nrequest_data = json.dumps(\n    {\n        \"id\": \"16\",\n        \"inputs\": [\n            {\"name\": \"data\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [b64_audio_data]},\n        ],\n    }\n)\nurl = \"http://localhost:8000/v2/models/ser/versions/1/infer\"\n\n# ...\n# Example output:\n# {\n#   \"id\": \"134\",\n#   \"model_name\": \"ser\",\n#   \"model_version\": \"1\",\n#   \"outputs\": [\n#     {\n#       \"name\": \"results\",\n#       \"datatype\": \"BYTES\",\n#       \"shape\": [],\n#       \"data\": [\n#         \"[{\\\"score\\\": 0.9841493964195251, \\\"label\\\": \\\"happy\\\"}, {\\\"score\\\": 0.012710321694612503, \\\"label\\\": \\\"disgust\\\"}, {\\\"score\\\": 0.0015516174025833607, \\\"label\\\": \\\"angry\\\"}, {\\\"score\\\": 0.0011115961242467165, \\\"label\\\": \\\"fear\\\"}, {\\\"score\\\": 0.0002755543973762542, \\\"label\\\": \\\"surprise\\\"}]\"\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"api/#fer-facial-emotion-recognition","title":"FER (Facial Emotion Recognition)","text":"<p>URL: <code>/v2/models/fer/versions/1/infer</code></p> <p>Input Names:</p> <ul> <li><code>data</code>: Base64 encoded image data (of jpg/png format)</li> </ul> <p>Output Names:</p> <ul> <li><code>results</code>: JSON dumped predictions with the <code>label</code> and <code>score</code> for each emotion</li> </ul> <pre><code># ...\nimage_data = open(\"path/to/image.jpg\", \"rb\").read()\nb64_image_data = base64.b64encode(image_data).decode(\"utf-8\")\nrequest_data = json.dumps(\n    {\n        \"id\": \"1\",\n        \"inputs\": [\n            {\"name\": \"data\", \"shape\": [1, 3, 224, 224], \"datatype\": \"BYTES\", \"data\": [b64_image_data]},\n        ],\n    }\n)\nurl = \"http://localhost:8000/v2/models/fer/versions/1/infer\"\n# ...\n# Example output:\n# {\n#   \"id\": \"108\",\n#   \"model_name\": \"fer\",\n#   \"model_version\": \"1\",\n#   \"outputs\": [\n#     {\n#       \"name\": \"results\",\n#       \"datatype\": \"BYTES\",\n#       \"shape\": [],\n#       \"data\": [\n#         \"[{\\\"label\\\": \\\"angry\\\", \\\"score\\\": 9.990368239735993e-07}, {\\\"label\\\": \\\"disgust\\\", \\\"score\\\": 3.2478467246928735e-12}, {\\\"label\\\": \\\"fear\\\", \\\"score\\\": 1.9463645189950467e-07}, {\\\"label\\\": \\\"happy\\\", \\\"score\\\": 99.98987317024906}, {\\\"label\\\": \\\"sad\\\", \\\"score\\\": 2.089022626165558e-05}, {\\\"label\\\": \\\"surprise\\\", \\\"score\\\": 1.4230729443738787e-05}, {\\\"label\\\": \\\"neutral\\\", \\\"score\\\": 0.010093500042319301}]\"\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"api/#nlp-natural-language-processing","title":"NLP (Natural Language Processing)","text":"<p>URL: <code>/v2/models/nlp/versions/1/infer</code></p> <p>Input Names:</p> <ul> <li><code>data</code>: Text data</li> </ul> <p>Output Names:</p> <ul> <li><code>results</code>: JSON dumped predictions with the <code>label</code> and <code>score</code> for each emotion</li> </ul> <pre><code># ...\ntext_data = \"I am very happy today!\"\nrequest_data = json.dumps(\n    {\n        \"id\": \"1\",\n        \"inputs\": [\n            {\"name\": \"data\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [text_data]},\n        ],\n    }\n)\nurl = \"http://localhost:8000/v2/models/nlp/versions/1/infer\"\n# ...\n# Example output:\n# {\n#   \"id\": \"193\",\n#   \"model_name\": \"nlp\",\n#   \"model_version\": \"1\",\n#   \"outputs\": [\n#     {\n#       \"name\": \"results\",\n#       \"datatype\": \"BYTES\",\n#       \"shape\": [],\n#       \"data\": [\n#         \"[{\\\"label\\\": \\\"joy\\\", \\\"score\\\": 0.8967722058296204}, {\\\"label\\\": \\\"excitement\\\", \\\"score\\\": 0.037859223783016205}, {\\\"label\\\": \\\"admiration\\\", \\\"score\\\": 0.02824225090444088}, {\\\"label\\\": \\\"neutral\\\", \\\"score\\\": 0.027330709621310234}, {\\\"label\\\": \\\"gratitude\\\", \\\"score\\\": 0.02417466975748539}, {\\\"label\\\": \\\"relief\\\", \\\"score\\\": 0.022464321926236153}, {\\\"label\\\": \\\"approval\\\", \\\"score\\\": 0.021864689886569977}, {\\\"label\\\": \\\"love\\\", \\\"score\\\": 0.014073355123400688}, {\\\"label\\\": \\\"caring\\\", \\\"score\\\": 0.011970801278948784}, {\\\"label\\\": \\\"amusement\\\", \\\"score\\\": 0.009650727733969688}, {\\\"label\\\": \\\"optimism\\\", \\\"score\\\": 0.006543426308780909}, {\\\"label\\\": \\\"realization\\\", \\\"score\\\": 0.006097565405070782}, {\\\"label\\\": \\\"pride\\\", \\\"score\\\": 0.005770173855125904}, {\\\"label\\\": \\\"annoyance\\\", \\\"score\\\": 0.005515581928193569}, {\\\"label\\\": \\\"disapproval\\\", \\\"score\\\": 0.0043140980415046215}, {\\\"label\\\": \\\"confusion\\\", \\\"score\\\": 0.0036673692520707846}, {\\\"label\\\": \\\"sadness\\\", \\\"score\\\": 0.0036307028494775295}, {\\\"label\\\": \\\"anger\\\", \\\"score\\\": 0.003237350843846798}, {\\\"label\\\": \\\"desire\\\", \\\"score\\\": 0.0026491915341466665}, {\\\"label\\\": \\\"curiosity\\\", \\\"score\\\": 0.0022887929808348417}, {\\\"label\\\": \\\"surprise\\\", \\\"score\\\": 0.002227753633633256}, {\\\"label\\\": \\\"disappointment\\\", \\\"score\\\": 0.0017585484310984612}, {\\\"label\\\": \\\"nervousness\\\", \\\"score\\\": 0.0016981943044811487}, {\\\"label\\\": \\\"grief\\\", \\\"score\\\": 0.0011259763268753886}, {\\\"label\\\": \\\"remorse\\\", \\\"score\\\": 0.0009548053494654596}, {\\\"label\\\": \\\"fear\\\", \\\"score\\\": 0.0009162503411062062}, {\\\"label\\\": \\\"embarrassment\\\", \\\"score\\\": 0.0008813319727778435}, {\\\"label\\\": \\\"disgust\\\", \\\"score\\\": 0.0006478808936662972}]\"\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"models/","title":"Models","text":"<p>By default all models are enabled: ASR, FER, NLP, SER. You can disable one or more by setting the <code>ALIVE_MODELS</code> environment variable to a comma-separated list of the models you want to enable.</p> <p>Default: <code>ALIVE_MODELS=asr,fer,nlp,ser</code></p>"},{"location":"models/#automatic-speech-recognition-asr","title":"Automatic Speech Recognition (ASR)","text":"<p>Model: SYSTRAN/faster-whisper Default Model Size: <code>distil-large-v3</code> Override it with <code>ALIVE_MODELS_ASR_MODEL_SIZE</code> environment variable.</p> <p>ASR model inference callable.</p>"},{"location":"models/#app.models.asr.ASR_INPUTS","title":"<code>ASR_INPUTS = [Tensor(name='data', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.asr.ASR_OUTPUTS","title":"<code>ASR_OUTPUTS = [Tensor(name='results', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.asr.asr_infer_fn","title":"<code>asr_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.str_]]]</code>","text":"<p>Inference function for ASR model.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>List[Request]</code> <p>The requests.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, NDArray[str_]]]</code> <p>The inference results. The final text can be obtained by joining each segment's [\"text\"] field.</p> Source code in <code>app/models/asr.py</code> <pre><code>def asr_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.str_]]]:\n    \"\"\"Inference function for ASR model.\n\n    Parameters\n    ----------\n    requests : List[Request]\n        The requests.\n\n    Returns\n    -------\n    List[Dict[str, NDArray[np.str_]]]\n        The inference results.\n        The final text can be obtained by joining each segment's [\"text\"] field.\n    \"\"\"\n    speech_data = [request.data[\"data\"] for request in requests]\n    total = len(speech_data)\n    results = []\n    for index in range(total):\n        segments: List[Dict[str, Any]] = []\n        audio_data = speech_data[index]\n        try:\n            segments = get_transcription(audio_data)\n        except BaseException as exc:\n            LOG.error(\"Error transcribing audio: %s\", exc)\n        segments_dump = json.dumps(segments, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n        result = {\n            \"results\": np.array(segments_dump, dtype=bytes),\n        }\n        results.append(result)\n    return results\n</code></pre>"},{"location":"models/#app.models.asr.get_transcription","title":"<code>get_transcription(audio_data: NDArray[Any]) -&gt; List[Dict[str, Any]]</code>","text":"<p>Transcribe audio data.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>NDArray[Any]</code> <p>The audio data.</p> required <p>Returns:</p> Type Description <code>Tuple[str, List[Dict[str, Any]]]</code> <p>The text and segments.</p> Source code in <code>app/models/asr.py</code> <pre><code>def get_transcription(\n    audio_data: NDArray[Any],\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Transcribe audio data.\n\n    Parameters\n    ----------\n    audio_data : NDArray[Any]\n        The audio data.\n\n    Returns\n    -------\n    Tuple[str, List[Dict[str, Any]]]\n        The text and segments.\n    \"\"\"\n    base64_data = np.char.decode(audio_data.astype(\"bytes\"), \"utf-8\")\n    try:\n        wav_data = base64.b64decode(base64_data)\n    except BaseException:\n        return []\n    if not wav_data:\n        return []\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_file:\n        temp_file.write(wav_data)\n        temp_file.flush()\n        segments_iter, _ = model.transcribe(\n            audio=temp_file.name,\n            language=\"en\",\n            task=\"transcribe\",\n            beam_size=5,\n            condition_on_previous_text=False,\n            initial_prompt=None,\n            word_timestamps=True,\n            vad_filter=True,\n            hallucination_silence_threshold=0.3,\n        )\n        segments = list(segments_iter)\n        segment_dicts = _segments_to_dicts(segments)\n    return segment_dicts\n</code></pre>"},{"location":"models/#facial-emotion-recognition-fer","title":"Facial Emotion Recognition (FER)","text":"<p>Library: serengil/deepface Default detector backend: <code>yolov8</code> Override it with <code>ALIVE_MODELS_FER_MODEL_DETECTOR_BACKEND</code> environment variable.</p> <p>FER model inference callable.</p>"},{"location":"models/#app.models.fer.FER_INPUTS","title":"<code>FER_INPUTS: List[Tensor] = [Tensor(name='data', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.fer.FER_OUTPUTS","title":"<code>FER_OUTPUTS: List[Tensor] = [Tensor(name='results', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.fer.fer_infer_fn","title":"<code>fer_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]</code>","text":"<p>Inference function for FER model.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>List[Request]</code> <p>The requests.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, NDArray[int_] | NDArray[float_]]]</code> <p>The inference results.</p> Source code in <code>app/models/fer.py</code> <pre><code>def fer_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]:\n    \"\"\"Inference function for FER model.\n\n    Parameters\n    ----------\n    requests : List[Request]\n        The requests.\n\n    Returns\n    -------\n    List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]\n        The inference results.\n    \"\"\"\n    infer_inputs = [request.data[\"data\"] for request in requests]\n    total = len(infer_inputs)\n    results = []\n    for index in range(total):\n        data_string = infer_inputs[index]\n        analysis_results = get_image_analysis(data_string)\n        result = {\n            \"results\": np.array(json.dumps(analysis_results), dtype=bytes),\n        }\n        results.append(result)\n    return results\n</code></pre>"},{"location":"models/#app.models.fer.get_image_analysis","title":"<code>get_image_analysis(data_string: NDArray[Any]) -&gt; List[Dict[str, str | float]]</code>","text":"<p>Analyze an image.</p> <p>Parameters:</p> Name Type Description Default <code>data_string</code> <code>NDArray[Any]</code> <p>The image data as a string.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, str | float]]</code> <p>The predicted labels and scores.</p> Source code in <code>app/models/fer.py</code> <pre><code>def get_image_analysis(data_string: NDArray[Any]) -&gt; List[Dict[str, str | float]]:\n    \"\"\"Analyze an image.\n\n    Parameters\n    ----------\n    data_string : NDArray[Any]\n        The image data as a string.\n\n    Returns\n    -------\n    List[Dict[str, str | float]]\n        The predicted labels and scores.\n    \"\"\"\n    try:\n        base64_data = np.char.decode(data_string.astype(\"bytes\"), \"utf-8\")\n        img_data = base64.b64decode(base64_data)\n        with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=True) as temp_file:\n            temp_file.write(img_data)\n            temp_file.flush()\n            analysis_results = DeepFace.analyze(\n                temp_file.name,\n                actions=[\"emotion\"],\n                detector_backend=FER_MODEL_DETECTOR_BACKEND,\n                enforce_detection=False,\n            )\n    except BaseException as exc:\n        LOG.error(\"Error analyzing file: %s\", exc)\n        return []\n    by_confidence = sorted(analysis_results, key=lambda x: x[\"face_confidence\"], reverse=True)\n    first_result = by_confidence[0]\n    face_confidence = float(first_result[\"face_confidence\"])\n    emotions = []\n    if face_confidence &gt; FER_MODEL_FACE_MIN_CONFIDENCE:\n        emotions_dict = first_result[\"emotion\"]\n        for emotion, score in emotions_dict.items():\n            emotions.append({\"label\": emotion, \"score\": score})\n    return emotions\n</code></pre>"},{"location":"models/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<p>Library: huggingface/transformers Default model: <code>SamLowe/roberta-base-go_emotions-onnx</code> file: <code>onnx/model_quantized.onnx</code> Override it with <code>ALIVE_MODELS_NLP_MODEL_REPO</code> and/or <code>ALIVE_MODELS_NLP_MODEL_FILE</code> environment variables.</p> <p>NLP model inference callable.</p>"},{"location":"models/#app.models.nlp.NLP_INPUTS","title":"<code>NLP_INPUTS = [Tensor(name='data', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.nlp.NLP_OUTPUTS","title":"<code>NLP_OUTPUTS = [Tensor(name='results', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.nlp.nlp_infer_fn","title":"<code>nlp_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]</code>","text":"<p>Inference function for NLP model.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>List[Request]</code> <p>The requests.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, NDArray[int_] | NDArray[float_]]]</code> <p>The inference results.</p> Source code in <code>app/models/nlp.py</code> <pre><code>def nlp_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]:\n    \"\"\"Inference function for NLP model.\n\n    Parameters\n    ----------\n    requests : List[Request]\n        The requests.\n\n    Returns\n    -------\n    List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]\n        The inference results.\n    \"\"\"\n    infer_inputs = [request.data[\"data\"] for request in requests]\n    total = len(infer_inputs)\n    results = []\n    for index in range(total):\n        input_text = infer_inputs[index]\n        analysis_results = get_text_sentiment(to_string(input_text))\n        result = {\n            \"results\": np.array(json.dumps(analysis_results), dtype=bytes),\n        }\n        results.append(result)\n    return results\n</code></pre>"},{"location":"models/#app.models.nlp.get_text_sentiment","title":"<code>get_text_sentiment(text: str) -&gt; List[Dict[str, str | float]]</code>","text":"<p>Get the sentiment of a text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to analyze.</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, float]]</code> <p>The sentiment labels and scores</p> Source code in <code>app/models/nlp.py</code> <pre><code>def get_text_sentiment(text: str) -&gt; List[Dict[str, str | float]]:\n    \"\"\"Get the sentiment of a text.\n\n    Parameters\n    ----------\n    text : str\n        The text to analyze.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        The sentiment labels and scores\n    \"\"\"\n    # pylint: disable=broad-except,too-many-try-statements\n    try:\n        outputs = classifier(text)\n        LOG.debug(\"Outputs: %s\", outputs)\n        # sorted_outputs = sorted(outputs, key=lambda x: x[0][\"score\"], reverse=True)\n        # prediction: Dict[str, str | float] = sorted_outputs[0][0]\n    except BaseException as exc:\n        LOG.error(\"Error getting prediction: %s\", exc)\n        outputs = []\n    return outputs[0]\n</code></pre>"},{"location":"models/#speech-emotion-recognition-ser","title":"Speech Emotion Recognition (SER)","text":"<p>SER model inference callable.</p>"},{"location":"models/#app.models.ser.SER_INPUTS","title":"<code>SER_INPUTS = [Tensor(name='data', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.ser.SER_OUTPUTS","title":"<code>SER_OUTPUTS = [Tensor(name='results', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.ser.ser_infer_fn","title":"<code>ser_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]</code>","text":"<p>Inference function for SER model.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>List[Request]</code> <p>The requests.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, NDArray[int_] | NDArray[float_]]]</code> <p>The inference results.</p> Source code in <code>app/models/ser.py</code> <pre><code>def ser_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]:\n    \"\"\"Inference function for SER model.\n\n    Parameters\n    ----------\n    requests : List[Request]\n        The requests.\n\n    Returns\n    -------\n    List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]\n        The inference results.\n    \"\"\"\n    infer_inputs = [request.data[\"data\"] for request in requests]\n    total = len(infer_inputs)\n    results = []\n    for index in range(total):\n        data_string = infer_inputs[index]\n        try:\n            base64_data = np.char.decode(data_string.astype(\"bytes\"), \"utf-8\")\n            wav_data = base64.b64decode(base64_data)\n            analysis_results = get_audio_analysis(wav_data)\n        except BaseException as exc:\n            LOG.error(\"Error analyzing file: %s\", exc)\n            analysis_results = []\n        result = {\"results\": np.array(json.dumps(analysis_results), dtype=bytes)}\n        results.append(result)\n    return results\n</code></pre>"},{"location":"models/#app.models.ser.get_audio_analysis","title":"<code>get_audio_analysis(audio_data: bytes) -&gt; List[Dict[str, str | float]]</code>","text":"<p>Get the prediction from the model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The wav audio data.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, str | float]]</code> <p>The predicted labels and scores.</p> Source code in <code>app/models/ser.py</code> <pre><code>def get_audio_analysis(audio_data: bytes) -&gt; List[Dict[str, str | float]]:\n    \"\"\"Get the prediction from the model.\n\n    Parameters\n    ----------\n    data : bytes\n        The wav audio data.\n\n    Returns\n    -------\n    List[Dict[str, str | float]]\n        The predicted labels and scores.\n    \"\"\"\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_file:\n        temp_file.write(audio_data)\n        temp_file.flush()\n        outputs = classifier(temp_file.name)  # type: ignore\n    LOG.debug(\"Outputs: %s\", outputs)\n    return outputs\n</code></pre>"}]}