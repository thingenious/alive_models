{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Alive Models Server","text":"<p>A PyTriton server of multiple models for different tasks like Automatic Speech Recognition, Speech Emotion Recognition, Facial Emotion Recognition, and Natural Language Processing (Sentiment Analysis) Small chunks of audio and text, as well as single images, can be sent to the server to get the results of the models. The server can be deployed using Podman/Docker compose or Kubernetes and exposed to the internet using a reverse proxy like Nginx. The exposed REST and gRPC endpoints follow the Predict Inference Protocol, version 2.</p>"},{"location":"#demo","title":"Demo","text":"<p>One or more of the models can be used to analyze audio, text, and images. A short capture of an example that uses all the models can be seen below:</p>"},{"location":"#models","title":"Models","text":"<p>The models that are currently being served are:</p> <ul> <li>ASR (Automatic Speech Recognition): distil-large-v3 using faster-whisper</li> <li>FER (Facial Emotion Recognition): deepface with yolov8 for face detection.</li> <li>NLP (Natural Language Processing): SamLowe/roberta-base-go_emotions-onnx</li> <li>SER (Speech Emotion Recognition): hughlan1214/Speech_Emotion_Recognition_wav2vec2-large-xlsr-53_240304_SER_fine-tuned2.0</li> </ul>"},{"location":"api/","title":"API","text":"<p>For details on all the available endpoints, you can refer to the standard inference protocols and the PyTriton docs.</p>"},{"location":"api/#rest-examples","title":"REST Examples","text":"<p>Some examples of how to use the REST API are shown below.</p>"},{"location":"api/#asr-automatic-speech-recognition","title":"ASR (Automatic Speech Recognition)","text":"<p>URL: <code>/v2/models/asr/versions/1/infer</code> Input Names:</p> <ul> <li><code>data</code>: Base64 encoded audio data (of wav in 16-bit PCM format)</li> <li><code>initial_prompt</code>: Initial prompt for the ASR model (can be an empty string)</li> </ul> <p>Output Names:</p> <ul> <li><code>text</code>: Transcription of the audio</li> <li><code>segments</code>: Segments of the audio with timestamps and words</li> </ul> <pre><code>import base64\nimport json\nimport httpx\n\nheaders = {\"Content-Type\": \"application/json\"}\ninitial_prompt = \"\"\naudio_data = open(\"path/to/audio.wav\", \"rb\").read()\nb64_audio_data = base64.b64encode(audio_data).decode(\"utf-8\")\nrequest_data = json.dumps(\n    {\n        \"id\": \"1\",\n        \"inputs\": [\n            {\"name\": \"data\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [b64_audio_data]},\n            {\"name\": \"initial_prompt\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [initial_prompt]},\n        ],\n    }\n)\nurl = \"http://localhost:8000/v2/models/asr/versions/1/infer\"\nresponse = httpx.post(\n    url,\n    headers=headers,\n    content=request_data,\n    timeout=60,\n)\n\nprint(response.json())\n\n# Example Output:\n# {\n#     \"id\": \"1\",\n#     \"model_name\": \"asr\",\n#     \"model_version\": \"1\",\n#     \"outputs\": [\n#         {\n#             \"name\": \"text\",\n#             \"datatype\": \"BYTES\",\n#             \"shape\": [1],\n#             \"data\": [\"hello world\"]\n#         },\n#         {\n#             \"name\": \"segments\",\n#             \"datatype\": \"BYTES\",\n#             \"shape\": [1],\n#             \"data\": [\"[{\\\"id\\\": 1, \\\"seek\\\": 10, \\\"start\\\": 0.0, \\\"end\\\": 1.0, \\\"text\\\": \\\"hello\\\", \\\"tokens\\\": [1, 2, 3, 4, 5], \\\"temperature\\\": 0.0, \\\"avg_logprob\\\": -0.1140624976158142, \\\"compression_ratio\\\": 1.0, \\\"no_speech_prob\\\": 0.0011053085327148438, \\\"words\\\": [{\\\"start\\\": 0.0, \\\"end\\\": 0.6, \\\"word\\\": \\\"Hello\\\", \\\"probability\\\": 0.96728515625}, {\\\"start\\\": 0.7, \\\"end\\\": 1.2, \\\"word\\\": \\\"World\\\", \\\"probability\\\": 0.99755859375]\"]\n#         }\n#     ]\n# }\n</code></pre>"},{"location":"api/#ser-speech-emotion-recognition","title":"SER (Speech Emotion Recognition)","text":"<p>URL: <code>/v2/models/ser/versions/1/infer</code></p> <p>Input Names:</p> <ul> <li><code>data</code>: Base64 encoded audio data (of wav in 16-bit PCM format)</li> </ul> <p>Output Names:</p> <ul> <li><code>label</code>: Emotion label</li> <li><code>score</code>: Confidence score</li> </ul> <pre><code># ...\nrequest_data = json.dumps(\n    {\n        \"id\": \"16\",\n        \"inputs\": [\n            {\"name\": \"data\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [b64_audio_data]},\n        ],\n    }\n)\nurl = \"http://localhost:8000/v2/models/ser/versions/1/infer\"\n# ...\n# Example Output:\n#\n# {\n#     \"id\": \"16\"\n#     \"model_name\": \"ser\",\n#     \"model_version\": \"1\",\n#     \"outputs\": [\n#         {\n#             \"name\": \"label\",\n#             \"datatype\": \"BYTES\",\n#             \"shape\": [1],\n#             \"data\": [\"happy\"]\n#         },\n#         {\n#             \"name\": \"score\",\n#             \"datatype\": \"FP32\",\n#             \"shape\": [],\n#             \"data\": [0.8824779987335205]\n#         }\n#     ]\n# }\n</code></pre>"},{"location":"api/#fer-facial-emotion-recognition","title":"FER (Facial Emotion Recognition)","text":"<p>URL: <code>/v2/models/fer/versions/1/infer</code></p> <p>Input Names:</p> <ul> <li><code>data</code>: Base64 encoded image data (of jpg/png format)</li> </ul> <p>Output Names:</p> <ul> <li><code>label</code>: Emotion label</li> <li><code>score</code>: Confidence score</li> </ul> <pre><code># ...\nimage_data = open(\"path/to/image.jpg\", \"rb\").read()\nb64_image_data = base64.b64encode(image_data).decode(\"utf-8\")\nrequest_data = json.dumps(\n    {\n        \"id\": \"1\",\n        \"inputs\": [\n            {\"name\": \"data\", \"shape\": [1, 3, 224, 224], \"datatype\": \"BYTES\", \"data\": [b64_image_data]},\n        ],\n    }\n)\nurl = \"http://localhost:8000/v2/models/fer/versions/1/infer\"\n# ...\n# Same output format as above (label and score)\n</code></pre>"},{"location":"api/#nlp-natural-language-processing","title":"NLP (Natural Language Processing)","text":"<p>URL: <code>/v2/models/nlp/versions/1/infer</code></p> <p>Input Names:</p> <ul> <li><code>text</code>: Text data</li> </ul> <p>Output Names:</p> <ul> <li><code>label</code>: Emotion label</li> <li><code>score</code>: Confidence score</li> </ul> <pre><code># ...\ntext_data = \"I am very happy today!\"\nrequest_data = json.dumps(\n    {\n        \"id\": \"1\",\n        \"inputs\": [\n            {\"name\": \"text\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [text_data]},\n        ],\n    }\n)\nurl = \"http://localhost:8000/v2/models/nlp/versions/1/infer\"\n# ...\n# Same output format as above (label and score)\n</code></pre>"},{"location":"models/","title":"Models","text":"<p>By default all models are enabled: ASR, FER, NLP, SER. You can disable one or more by setting the <code>ALIVE_MODELS</code> environment variable to a comma-separated list of the models you want to enable.</p> <p>Default: <code>ALIVE_MODELS=asr,fer,nlp,ser</code></p>"},{"location":"models/#automatic-speech-recognition-asr","title":"Automatic Speech Recognition (ASR)","text":"<p>Model: SYSTRAN/faster-whisper Default Model Size: <code>distil-large-v3</code> Override it with <code>ALIVE_MODELS_ASR_MODEL_SIZE</code> environment variable.</p> <p>ASR model inference callable.</p>"},{"location":"models/#app.models.asr.ASR_INPUTS","title":"<code>ASR_INPUTS = [Tensor(name='data', dtype=bytes, shape=(1)), Tensor(name='initial_prompt', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.asr.ASR_OUTPUTS","title":"<code>ASR_OUTPUTS = [Tensor(name='text', dtype=bytes, shape=(1)), Tensor(name='segments', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.asr.asr_infer_fn","title":"<code>asr_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]</code>","text":"<p>Inference function for ASR model.</p> Source code in <code>app/models/asr.py</code> <pre><code>def asr_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]:\n    \"\"\"Inference function for ASR model.\"\"\"\n    speech_data = [request.data[\"data\"] for request in requests]\n    initial_prompts = [request.data[\"initial_prompt\"] for request in requests]\n    total = len(speech_data)\n    results = []\n    for index in range(total):\n        transcription = \"\"\n        segments = \"[]\"\n        audio_data = speech_data[index]\n        initial_prompt = initial_prompts[index]\n        try:\n            transcription, segments = get_transcription(audio_data, initial_prompt=_to_string(initial_prompt))\n        except BaseException as exc:\n            LOG.error(\"Error transcribing audio: %s\", exc)\n        result = {\n            \"text\": np.array([transcription], dtype=bytes),\n            \"segments\": np.array([segments], dtype=bytes),\n        }\n        results.append(result)\n    return results\n</code></pre>"},{"location":"models/#app.models.asr.get_transcription","title":"<code>get_transcription(audio_data: NDArray[Any], initial_prompt: str) -&gt; Tuple[str, str]</code>","text":"<p>Transcribe audio data.</p> Source code in <code>app/models/asr.py</code> <pre><code>def get_transcription(audio_data: NDArray[Any], initial_prompt: str) -&gt; Tuple[str, str]:\n    \"\"\"Transcribe audio data.\"\"\"\n    _initial_prompt: str | None = None\n    if initial_prompt:  # if empty string, use None\n        _initial_prompt = initial_prompt\n    base64_data = np.char.decode(audio_data.astype(\"bytes\"), \"utf-8\")\n    wav_data = base64.b64decode(base64_data.data)\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_file:\n        temp_file.write(wav_data)\n        temp_file.flush()\n        segments_iter, _ = model.transcribe(\n            audio=temp_file.name,\n            language=\"en\",\n            task=\"transcribe\",\n            beam_size=5,\n            condition_on_previous_text=False,\n            initial_prompt=_initial_prompt,\n            word_timestamps=True,\n            vad_filter=True,\n            hallucination_silence_threshold=0.3,\n        )\n        segments = list(segments_iter)\n        text = \"\".join([segment.text for segment in segments])\n        segment_dicts = _segments_to_dicts(segments)\n        segments_dump = json.dumps(segment_dicts, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n    return text, segments_dump\n</code></pre>"},{"location":"models/#facial-emotion-recognition-fer","title":"Facial Emotion Recognition (FER)","text":"<p>Library: serengil/deepface Default detector backend: <code>yolov8</code> Override it with <code>ALIVE_MODELS_FER_MODEL_DETECTOR_BACKEND</code> environment variable.</p> <p>FER model inference callable.</p>"},{"location":"models/#app.models.fer.FER_INPUTS","title":"<code>FER_INPUTS: List[Tensor] = [Tensor(name='data', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.fer.FER_OUTPUTS","title":"<code>FER_OUTPUTS: List[Tensor] = [Tensor(name='label', dtype=bytes, shape=(1)), Tensor(name='score', dtype=np.float32, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.fer.fer_infer_fn","title":"<code>fer_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]</code>","text":"<p>Inference function for FER model.</p> Source code in <code>app/models/fer.py</code> <pre><code>def fer_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]:\n    \"\"\"Inference function for FER model.\"\"\"\n    infer_inputs = [request.data[\"data\"] for request in requests]\n    total = len(infer_inputs)\n    results = []\n    for index in range(total):\n        data_string = infer_inputs[index]\n        label, score = get_image_analysis(data_string)\n        result = {\n            \"label\": np.char.encode(label, \"utf-8\"),\n            \"score\": np.array([score], dtype=np.float32),\n        }\n        results.append(result)\n    return results\n</code></pre>"},{"location":"models/#app.models.fer.get_image_analysis","title":"<code>get_image_analysis(data_string: NDArray[Any]) -&gt; Tuple[str, float]</code>","text":"<p>Analyze an image.</p> Source code in <code>app/models/fer.py</code> <pre><code>def get_image_analysis(data_string: NDArray[Any]) -&gt; Tuple[str, float]:\n    \"\"\"Analyze an image.\"\"\"\n    label = \"unknown\"\n    score = 0.0\n    try:\n        base64_data = np.char.decode(data_string.astype(\"bytes\"), \"utf-8\")\n        img_data = base64.b64decode(base64_data)\n        with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=True) as temp_file:\n            temp_file.write(img_data)\n            temp_file.flush()\n            analysis_results = DeepFace.analyze(\n                temp_file.name,\n                actions=[\"emotion\"],\n                detector_backend=FER_MODEL_DETECTOR_BACKEND,\n                enforce_detection=False,\n            )\n    except BaseException as exc:\n        LOG.error(\"Error analyzing file: %s\", exc)\n        return label, score\n    by_confidence = sorted(analysis_results, key=lambda x: x[\"face_confidence\"], reverse=True)\n    first_result = by_confidence[0]\n    face_confidence = float(first_result[\"face_confidence\"])\n    if face_confidence &gt; FER_MODEL_FACE_MIN_CONFIDENCE:\n        label = first_result[\"dominant_emotion\"]\n        score = first_result[\"emotion\"][label]\n    return label, score\n</code></pre>"},{"location":"models/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<p>Library: huggingface/transformers Default model: <code>SamLowe/roberta-base-go_emotions-onnx</code> file: <code>onnx/model_quantized.onnx</code> Override it with <code>ALIVE_MODELS_NLP_MODEL_REPO</code> and/or <code>ALIVE_MODELS_NLP_MODEL_FILE</code> environment variables.</p> <p>NLP model inference callable.</p>"},{"location":"models/#app.models.nlp.NLP_INPUTS","title":"<code>NLP_INPUTS = [Tensor(name='text', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.nlp.NLP_OUTPUTS","title":"<code>NLP_OUTPUTS = [Tensor(name='label', dtype=bytes, shape=(1)), Tensor(name='score', dtype=np.float32, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.nlp.nlp_infer_fn","title":"<code>nlp_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]</code>","text":"<p>Inference function for NLP model.</p> Source code in <code>app/models/nlp.py</code> <pre><code>def nlp_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]:\n    \"\"\"Inference function for NLP model.\"\"\"\n    infer_inputs = [request.data[\"text\"] for request in requests]\n    total = len(infer_inputs)\n    results = []\n    for index in range(total):\n        input_text = infer_inputs[index]\n        label, score = get_text_sentiment(input_text)\n        result = {\n            \"label\": np.char.encode(np.array(label), \"utf-8\"),\n            \"score\": np.array([score], dtype=np.float32),\n        }\n        results.append(result)\n    return results\n</code></pre>"},{"location":"models/#app.models.nlp.get_text_sentiment","title":"<code>get_text_sentiment(text: Any) -&gt; Tuple[str, float]</code>","text":"<p>Get the sentiment of a text.</p> Source code in <code>app/models/nlp.py</code> <pre><code>def get_text_sentiment(text: Any) -&gt; Tuple[str, float]:\n    \"\"\"Get the sentiment of a text.\"\"\"\n    label = \"unknown\"\n    score = 0.0\n    # pylint: disable=broad-except,too-many-try-statements\n    try:\n        outputs = classifier(_to_string(text))\n        sorted_outputs = sorted(outputs, key=lambda x: x[0][\"score\"], reverse=True)\n        prediction: Dict[str, str | float] = sorted_outputs[0][0]\n        label = str(prediction[\"label\"])\n        score = float(prediction[\"score\"])\n    except BaseException as exc:\n        LOG.error(\"Error getting prediction: %s\", exc)\n    return label, score\n</code></pre>"},{"location":"models/#speech-emotion-recognition-ser","title":"Speech Emotion Recognition (SER)","text":"<p>SER model inference callable.</p>"},{"location":"models/#app.models.ser.SER_INPUTS","title":"<code>SER_INPUTS = [Tensor(name='data', dtype=bytes, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.ser.SER_OUTPUTS","title":"<code>SER_OUTPUTS = [Tensor(name='label', dtype=bytes, shape=(1)), Tensor(name='score', dtype=np.float32, shape=(1))]</code>  <code>module-attribute</code>","text":""},{"location":"models/#app.models.ser.ser_infer_fn","title":"<code>ser_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]</code>","text":"<p>Inference function for SER model.</p> Source code in <code>app/models/ser.py</code> <pre><code>def ser_infer_fn(requests: List[Request]) -&gt; List[Dict[str, NDArray[np.int_] | NDArray[np.float_]]]:\n    \"\"\"Inference function for SER model.\"\"\"\n    infer_inputs = [request.data[\"data\"] for request in requests]\n    total = len(infer_inputs)\n    results = []\n    for index in range(total):\n        label = \"unknown\"\n        score = 0.0\n        data_string = infer_inputs[index]\n        try:\n            base64_data = np.char.decode(data_string.astype(\"bytes\"), \"utf-8\")\n            wav_data = base64.b64decode(base64_data)\n            label, score = get_audio_analysis(wav_data)\n        except BaseException as exc:\n            LOG.error(\"Error analyzing file: %s\", exc)\n        result = {\n            \"label\": np.char.encode(label, \"utf-8\"),\n            \"score\": np.array([score], dtype=np.float32),\n        }\n        results.append(result)\n    return results\n</code></pre>"},{"location":"models/#app.models.ser.get_audio_analysis","title":"<code>get_audio_analysis(data: bytes) -&gt; Tuple[str, float]</code>","text":"<p>Get the prediction from the model.</p> Source code in <code>app/models/ser.py</code> <pre><code>def get_audio_analysis(data: bytes) -&gt; Tuple[str, float]:\n    \"\"\"Get the prediction from the model.\"\"\"\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_file:\n        temp_file.write(data)\n        temp_file.flush()\n        outputs = classifier(temp_file.name)  # type: ignore\n    highest_score = max(outputs, key=lambda x: x[\"score\"])\n    label = highest_score[\"label\"]\n    score = highest_score[\"score\"]\n    return label, score\n</code></pre>"}]}