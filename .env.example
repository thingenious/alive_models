# ####################################################################
# App Config
# ####################################################################
ALIVE_MODELS=asr,fer,ser,nlp
# comma separated list of models to load
# empty string means load all models
# depending on the gpu memory and the models,
# we might not be able to load all models at the same machine
#
# the ports pytriton uses by default
#
ALIVE_MODELS_HTTP_PORT=8000
ALIVE_MODELS_GRPC_PORT=8001
ALIVE_MODELS_METRICS_PORT=8002
ALIVE_MODELS_SAGEMAKER_PORT=8080
#
# things we might want to change (regarding the models to use)
#
ALIVE_MODELS_ASR_MODEL_SIZE=distil-large-v3
# https://github.com/neuspell/neuspell
# only the ones that are also in hf (or none)
# e.g. subwordbert-probwordnoise, scrnn-probwordnoise
ALIVE_MODELS_ASR_CORRECTION_MODEL=subwordbert-probwordnoise
ALIVE_MODELS_FER_MODEL_DETECTOR_BACKEND=yolov8
ALIVE_MODELS_FER_MODEL_FACE_MIN_CONFIDENCE=0.7
ALIVE_MODELS_SER_MODEL_REPO=hughlan1214/Speech_Emotion_Recognition_wav2vec2-large-xlsr-53_240304_SER_fine-tuned2.0
ALIVE_MODELS_NLP_MODEL_REPO=SamLowe/roberta-base-go_emotions-onnx
ALIVE_MODELS_NLP_MODEL_FILE=onnx/model_quantized.onnx
#
# we probably don't want to need to change these
# they are for the url path construction: "/v2/models/{model_name}/versions/{model_version}/infer"
ALIVE_MODELS_ASR_MODEL_NAME=asr
ALIVE_MODELS_ASR_MODEL_VERSION=1
#
ALIVE_MODELS_FER_MODEL_NAME=fer
ALIVE_MODELS_FER_MODEL_VERSION=1
#
ALIVE_MODELS_SER_MODEL_NAME=ser
ALIVE_MODELS_SER_MODEL_VERSION=1
#
ALIVE_MODELS_NLP_MODEL_NAME=nlp
ALIVE_MODELS_NLP_MODEL_VERSION=1
#
# other pytriton related vars
#
ALIVE_MODELS_ALLOW_HTTP=true
ALIVE_MODELS_ALLOW_GRPC=true
ALIVE_MODELS_ALLOW_METRICS=true
ALIVE_MODELS_ALLOW_SAGEMAKER=true
#
ALIVE_MODELS_GRPC_USE_SSL=false
ALIVE_MODELS_GRPC_USE_SSL_MUTUAL=false
# not supported yet
# if we use ssl, we should check the paths (host vs container/volume paths)
ALIVE_MODELS_GRPC_ROOT_CERT=
ALIVE_MODELS_GRPC_SERVER_CERT=
ALIVE_MODELS_GRPC_SERVER_KEY=
#
# ####################################################################
# Build and Deployment
# ####################################################################
#
# Containers/compose
# ####################################################################
# podman or docker
CONTAINER_COMMAND=podman
CONTAINER_IMAGE=localhost:5000/alive_models
# the name to use for compose or plain docker/podman commands
CONTAINER_NAME=alive_models
# leave the tag empty to make one based on the base nvcr.io/nvidia/... image (cuda version)
# e.g. generated (cuda) tag: 12.4.1-devel-ubuntu22.04
# generated: CONTAINER_TAG=0.0.1-cuda-12.4.1
CONTAINER_TAG=0.0.1-cuda-12.4.1
# tags for base image:
# https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda/tags
# nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04
# nvcr.io/nvidia/cuda:12.3.2-devel-ubuntu22.04
# nvcr.io/nvidia/cuda:12.2.2-devel-ubuntu22.04
# nvcr.io/nvidia/cuda:12.1.1-devel-ubuntu22.04
#
# default/latest: nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04
ALIVE_MODELS_BASE_IMAGE=nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04
#
# ##################################################################
# helm/k8s
# things to override if needed in the helm chart
# case sensitive after the `K8S_HELM_` prefix
# to use them directly in `helm template --set` command
# ###################################################################
HELM_TEMPLATE_NAME=alive
K8S_HELM_namespace=alive
K8S_HELM_deployment.replicaCount=1
K8S_HELM_deployment.image.repository=localhost:5000/alive_models
K8S_HELM_deployment.image.tag=0.0.1-cuda-12.4.1
K8S_HELM_deployment.image.pullSecret=
K8S_HELM_deployment.image.pullPolicy=Always
K8S_HELM_deployment.restartPolicy=Always
K8S_HELM_deployment.failureThreshold=10
K8S_HELM_deployment.initialDelaySeconds=30
K8S_HELM_deployment.periodSeconds=10
K8S_HELM_deployment.timeoutSeconds=5
K8S_HELM_deployment.resources.useGPU=true
K8S_HELM_deployment.resources.useMIG=false
K8S_HELM_deployment.resources.numGPUs=1
K8S_HELM_deployment.resources.migConfig.profile=mig-7g.40gb
K8S_HELM_deployment.resources.requests.memory=2Gi
K8S_HELM_deployment.resources.limits.memory=4Gi
K8S_HELM_deployment.resources.requests.cpu=1
K8S_HELM_deployment.resources.limits.cpu=2
K8S_HELM_service.type=LoadBalancer
K8S_HELM_service.http.enabled=true
K8S_HELM_service.http.nodePort=30111
K8S_HELM_service.grpc.enabled=true
K8S_HELM_service.grpc.nodePort=30112
K8S_HELM_service.metrics.enabled=true
K8S_HELM_service.metrics.nodePort=30113
K8S_HELM_service.sagemaker.enabled=false
K8S_HELM_service.sagemaker.nodePort=30114
K8S_HELM_pvc.enabled=true
K8S_HELM_pvc.accessMode=ReadWriteOnce
K8S_HELM_pvc.size=30Gi
