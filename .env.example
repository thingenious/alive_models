# ####################################################################
# App Config
# ####################################################################
ALIVE_MODELS=asr,fer,ser,nlp,tts
# comma separated list of models to load
# empty string means load all models
# depending on the gpu memory and the models,
# we might not be able to load all models at the same machine
#
# the ports pytriton uses by default
#
ALIVE_MODELS_HTTP_PORT=8000
ALIVE_MODELS_GRPC_PORT=8001
ALIVE_MODELS_METRICS_PORT=8002
ALIVE_MODELS_SAGEMAKER_PORT=8080
#
# things we might want to change (regarding the models to use)
#
ALIVE_MODELS_ASR_MODEL_SIZE=distil-large-v3
ALIVE_MODELS_FER_MODEL_DETECTOR_BACKEND=yolov8
ALIVE_MODELS_FER_MODEL_FACE_MIN_CONFIDENCE=0.7
ALIVE_MODELS_SER_MODEL_REPO=hughlan1214/Speech_Emotion_Recognition_wav2vec2-large-xlsr-53_240304_SER_fine-tuned2.0
ALIVE_MODELS_NLP_MODEL_REPO=SamLowe/roberta-base-go_emotions-onnx
ALIVE_MODELS_NLP_MODEL_FILE=onnx/model_quantized.onnx
ALIVE_MODELS_LID_MODEL_REPO=cis-lmu/glotlid
ALIVE_MODELS_LID_MODEL_FILE=model.bin
ALIVE_MODELS_TTS_MODEL_REPO=microsoft/speecht5_tts
#
# Other repos to try:
# ALIVE_MODELS_LID_MODEL_REPO=facebook/fasttext-language-identification
# ALIVE_MODELS_TTS_MODEL_REPO=microsoft/speecht5_tts
# ALIVE_MODELS_TTS_MODEL_REPO=facebook/fastspeech2-en-ljspeech
# ALIVE_MODELS_TTS_MODEL_REPO=facebook/fastspeech2-en-200_speaker-cv4
# ALIVE_MODELS_TTS_MODEL_REPO=suno/bark-small
# ALIVE_MODELS_TTS_MODEL_REPO=collabora/whisperspeech:s2a-q4-tiny-en+pl.model
# ALIVE_MODELS_TTS_MODEL_REPO=parler-tts/parler_tts_mini_v0.1
# ALIVE_MODELS_TTS_MODEL_REPO=facebook/hf-seamless-m4t-medium
# ALIVE_MODELS_TTS_MODEL_REPO=seamless-m4t-v2-large
# we probably don't want to need to change these
# they are for the url path construction: "/v2/models/{model_name}/versions/{model_version}/infer"
ALIVE_MODELS_ASR_MODEL_NAME=asr
ALIVE_MODELS_ASR_MODEL_VERSION=1
#
ALIVE_MODELS_FER_MODEL_NAME=fer
ALIVE_MODELS_FER_MODEL_VERSION=1
#
ALIVE_MODELS_SER_MODEL_NAME=ser
ALIVE_MODELS_SER_MODEL_VERSION=1
#
ALIVE_MODELS_NLP_MODEL_NAME=nlp
ALIVE_MODELS_NLP_MODEL_VERSION=1
#
ALIVE_MODELS_TTS_MODEL_NAME=tts
ALIVE_MODELS_TTS_MODEL_VERSION=1
#
ALIVE_MODELS_LID_MODEL_NAME=lid
ALIVE_MODELS_LID_MODEL_VERSION=1
#
# other pytriton related vars
#
ALIVE_MODELS_ALLOW_HTTP=true
ALIVE_MODELS_ALLOW_GRPC=true
ALIVE_MODELS_ALLOW_METRICS=true
ALIVE_MODELS_ALLOW_SAGEMAKER=true
#
ALIVE_MODELS_GRPC_USE_SSL=false
ALIVE_MODELS_GRPC_USE_SSL_MUTUAL=false
# not supported yet
# if we use ssl, we should check the paths (host vs container/volume paths)
ALIVE_MODELS_GRPC_ROOT_CERT=
ALIVE_MODELS_GRPC_SERVER_CERT=
ALIVE_MODELS_GRPC_SERVER_KEY=
#
# ####################################################################
# Build and Deployment
# ####################################################################
#
# Containers/compose
# ####################################################################
# podman or docker
CONTAINER_COMMAND=podman
CONTAINER_IMAGE=localhost:5000/alive_models
# the name to use for compose or plain docker/podman commands
CONTAINER_NAME=alive_models
# leave the tag empty to make one based on the base nvcr.io/nvidia/... image (cuda version)
# e.g. generated (cuda) tag: 12.4.1-devel-ubuntu22.04
# generated: CONTAINER_TAG=0.0.1-cuda-12.4.1
CONTAINER_TAG=
# tags for base image:
# https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda/tags
# nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04
# nvcr.io/nvidia/cuda:12.3.2-devel-ubuntu22.04
# nvcr.io/nvidia/cuda:12.2.2-devel-ubuntu22.04
# nvcr.io/nvidia/cuda:12.1.1-devel-ubuntu22.04
#
# default/latest: nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04
ALIVE_MODELS_BASE_IMAGE=nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04
#
# ##################################################################
# helm/k8s
# things to override if needed in the helm chart
# case sensitive after the `K8S_HELM_` prefix
# to use them directly in `helm template --set` command
# ###################################################################
HELM_TEMPLATE_NAME=alive
K8S_HELM_namespace=alive
K8S_HELM_deployment_replicaCount=1
K8S_HELM_deployment_image_repository=localhost:5000/alive_models
K8S_HELM_deployment_image_tag=0.0.1-cuda-12.4.1
K8S_HELM_deployment_image_pullSecret=
K8S_HELM_deployment_image_pullPolicy=Always
K8S_HELM_deployment_restartPolicy=Always
K8S_HELM_deployment_failureThreshold=10
K8S_HELM_deployment_initialDelaySeconds=30
K8S_HELM_deployment_periodSeconds=10
K8S_HELM_deployment_timeoutSeconds=5
K8S_HELM_deployment_resources_useGPU=true
K8S_HELM_deployment_resources_useMIG=false
K8S_HELM_deployment_resources_numGPUs=1
K8S_HELM_deployment_resources_migConfig_profile=mig-7g.40gb
K8S_HELM_deployment_resources_requests_memory=2Gi
K8S_HELM_deployment_resources_limits_memory=4Gi
K8S_HELM_deployment_resources_requests_cpu=1
K8S_HELM_deployment_resources_limits_cpu=2
K8S_HELM_service_type=LoadBalancer
K8S_HELM_service_http_enabled=true
K8S_HELM_service_http_nodePort=30111
K8S_HELM_service_grpc_enabled=true
K8S_HELM_service_grpc_nodePort=30112
K8S_HELM_service_metrics_enabled=true
K8S_HELM_service_metrics_nodePort=30113
K8S_HELM_service_sagemaker_enabled=false
K8S_HELM_service_sagemaker_nodePort=30114
K8S_HELM_pvc_enabled=true
K8S_HELM_pvc_accessMode=ReadWriteOnce
K8S_HELM_pvc_size=30Gi
